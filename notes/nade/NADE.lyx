#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Notes on NADE
\end_layout

\begin_layout Author
Alvin Chua
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The NADE (neural autoregressive distribution estimator) model acts on binarized
 ordered inputs 
\begin_inset Formula $v=(v_{1},v_{2},\cdots,v_{n})$
\end_inset

 , where 
\begin_inset Formula $v_{i}\in\{0,1\}$
\end_inset

; and gives an estimate for its probability 
\begin_inset Formula $p(v)$
\end_inset

 after it is trained on examples.
 The model is autoregressive, which means that we estimate 
\begin_inset Formula $p(v)$
\end_inset

 from a series of conditional probabilities 
\begin_inset Formula $p(v_{i}\vert v_{<i})$
\end_inset

, where 
\begin_inset Formula $i\in\{1,\cdots,n\}$
\end_inset

 and 
\begin_inset Formula $v_{<i}=(v_{1},v_{2},\cdots,v_{i-1})$
\end_inset

 are all inputs before 
\begin_inset Formula $v_{i}$
\end_inset

.
 It is also useful to specify that 
\begin_inset Formula $p(v_{1}\vert v_{<1})=p(v_{1})$
\end_inset

.
 The value of 
\begin_inset Formula $p(v)$
\end_inset

 is related to 
\begin_inset Formula $p(v_{i}\vert v_{<i})$
\end_inset

 by probability chain rule 
\begin_inset Formula 
\begin{equation}
p(v)=\prod_{i=1}^{n}p(v_{i}\vert v_{<i}),\label{eq:Chain}
\end{equation}

\end_inset

or similarly in terms of log probabilities 
\begin_inset Formula 
\begin{equation}
-\ln p(v)=-\sum_{i=1}^{n}\ln p(v_{i}\vert v_{<i}).\label{eq:LogChain}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
A Fully Visible Model
\end_layout

\begin_layout Standard
To understand NADE, it is useful to first discuss the fully visible sigmoid
 belief network.
 Here, we treat each each 
\begin_inset Formula $p(v_{i}\vert v_{<i})$
\end_inset

 as a sigmoid unit such that
\begin_inset Formula 
\begin{align*}
p_{i} & =\sigma\Big(c_{i}+\sum_{j=1}^{n}W_{ij}v_{j}\Big)\\
 & =\sigma\Big(c_{i}+\sum_{j<i}W_{ij}v_{j}\Big),
\end{align*}

\end_inset

where 
\begin_inset Formula $\sigma(x)$
\end_inset

 is an activation function, and the autoregressivity is enforced by requiring
 that 
\begin_inset Formula $W_{ij}$
\end_inset

 is a lower triangular 
\begin_inset Formula $n\times n$
\end_inset

 matrix such that 
\begin_inset Formula $W_{ij}=0$
\end_inset

 whenever 
\begin_inset Formula $j\ge i$
\end_inset

.
 As such, 
\begin_inset Formula $c_{i}$
\end_inset

 contributes 
\begin_inset Formula $n$
\end_inset

 parameters and 
\begin_inset Formula $W_{ij}$
\end_inset

 contributes 
\begin_inset Formula $n(n-1)/2$
\end_inset

 to the model.
 The value 
\begin_inset Formula $p_{i}$
\end_inset

 represents the probability 
\begin_inset Formula $p(v_{i}=1\vert v_{<i})$
\end_inset

, and conversely, 
\begin_inset Formula $p(v_{i}=0\vert v_{<i})=(1-p_{i})$
\end_inset

.
 We can write this more succinctly as 
\begin_inset Formula 
\begin{equation}
p(v_{i}\vert v_{<i})=p_{i}^{v_{i}}(1-p_{i})^{1-v_{i}},\label{eq:Bernoulli}
\end{equation}

\end_inset

or similarly in terms of log probabilities
\begin_inset Formula 
\begin{equation}
\ln p(v_{i}\vert v_{<i})=v_{i}\ln p_{i}+(1-v_{i})\ln(1-p_{i}),\label{eq:LogBernoulli}
\end{equation}

\end_inset

where we have used the property that 
\begin_inset Formula $v_{i}$
\end_inset

 is a Bernoulli random variable that only admits 
\begin_inset Formula $v_{i}\in\{0,1\}$
\end_inset

.
\end_layout

\begin_layout Section
NADE
\end_layout

\begin_layout Standard
The NADE model estimates conditional probabilities 
\begin_inset Formula $p(v_{i}\vert v_{<i})$
\end_inset

 by introducing hidden units.
 We will only consider the simplest case where each term 
\begin_inset Formula $p(v_{i}|v_{<i})$
\end_inset

 is associated with a single sub-block comprising 
\begin_inset Formula $m$
\end_inset

 hidden units.
 We require that the number of hidden units must be at most the same as
 the number of visible units, 
\begin_inset Formula $m\le n$
\end_inset

.
 Allowing more hidden units introduces redundant structure to the network.
 
\end_layout

\begin_layout Standard
The hidden units are denoted by 
\begin_inset Formula $h^{(i)}=\big(h_{1}^{(i)},\cdots,h_{m}^{(i)}\big)$
\end_inset

, where the superscript 
\begin_inset Formula $i$
\end_inset

 refers to a particular hidden layer sub-block that we use to estimate 
\begin_inset Formula $p_{i}$
\end_inset

.
 We will first discuss the unconstrained network and thereafter invoke the
 autoregressivity property to inform us of how we can constrain the network
 to insert structure and reduce the dimensionality of the network.
 The unconstrained equations with a one hidden layer are 
\begin_inset Formula 
\begin{align}
p_{i} & =\sigma\Big(b^{(i)}+\sum_{j=1}^{m}V_{j}^{(i)}h_{j}^{(i)}\Big)\text{ and}\label{eq:NadeP}\\
h_{j}^{(i)} & =\sigma\Big(c_{j}^{(i)}+\sum_{k=1}^{n}W_{jk}^{(i)}v_{k}\Big),\label{eq:NadeHUnconstrained}
\end{align}

\end_inset

where 
\begin_inset Formula $c_{j}^{(i)}$
\end_inset

 are biases associated with each of the hidden units 
\begin_inset Formula $j$
\end_inset

 and model 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $W_{jk}^{(i)}$
\end_inset

 are weights, each comprising an 
\begin_inset Formula $m\times n$
\end_inset

 matrix.
 Similarly, the hidden units are linked to the output by the weights 
\begin_inset Formula $V_{j}^{(i)}$
\end_inset

 and biases 
\begin_inset Formula $b^{(i)}$
\end_inset

.
 These are in turn used as inputs to estimate 
\begin_inset Formula $p_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
We constrain the network by requiring autoregressivity.
 This manifests in tied weights that link the inputs 
\begin_inset Formula $v_{i}$
\end_inset

 to the hidden units in sub-blocks 
\begin_inset Formula $h_{j}^{(i)}$
\end_inset

.
 In particular, the autoregressive property suggests that all values with
 the superscript 
\begin_inset Formula $i$
\end_inset

 are dependent only on the inputs 
\begin_inset Formula $v_{<i}$
\end_inset

.
 As such, we can set 
\begin_inset Formula $W_{jk}^{(i)}=0$
\end_inset

 whenever 
\begin_inset Formula $k\ge i$
\end_inset

 .
 Furthermore, we require that weights linking input nodes to equivalent
 structural nodes in each sub-block of hidden units are tied.
 More precisely, we can express this by requiring 
\begin_inset Formula $W_{jk}^{(i)}=W_{jk}^{(l)}$
\end_inset

 for all 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $i<l$
\end_inset

 and 
\begin_inset Formula $k<i$
\end_inset

.
 In doing this, we can now construct all weight matrices 
\begin_inset Formula $W_{jk}^{(i)}$
\end_inset

, where 
\begin_inset Formula $i<n$
\end_inset

 and 
\begin_inset Formula $n$
\end_inset

 is the number of visible units by setting 
\begin_inset Formula 
\[
W_{jk}^{(i)}=\begin{cases}
W_{jk} & k<i\text{ and}\\
0 & \text{otherwise}
\end{cases},
\]

\end_inset

where drop the superscript and write 
\begin_inset Formula $W_{jk}=W_{jk}^{(n)}$
\end_inset

.
 In the same spirit, we will also share biases across hidden units by requiring
 
\begin_inset Formula $c_{j}^{(i)}=c_{j}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 The NADE model has these adjustments that introduce autoregressivity.
 The equation linking the inputs to the hidden units is now
\begin_inset Formula 
\begin{equation}
h_{j}^{(i)}=\sigma\Big(c_{j}+\sum_{k=1}^{i-1}W_{jk}v_{k}\Big),\label{eq:NadeH}
\end{equation}

\end_inset

where equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:NadeH"

\end_inset

) replaces the unconstrained equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:NadeHUnconstrained"

\end_inset

).
 Figure (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NADE"

\end_inset

) illustrates the NADE model graphically.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/alschua/Downloads/NadeGraph (1).jpg
	scale 38

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
A NADE network with 4 visible and 2 hidden units.
 Common colours and line styles linking the input to hidden units indicate
 tied weights.
\begin_inset CommandInset label
LatexCommand label
name "fig:NADE"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can now estimate the density of 
\begin_inset Formula $p(v)$
\end_inset

 by first calculating 
\begin_inset Formula $h_{j}^{(i)}$
\end_inset

 and subsequently 
\begin_inset Formula $p_{i}$
\end_inset

, and folding this into 
\begin_inset Formula $p(v)$
\end_inset

 using equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Bernoulli"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Chain"

\end_inset

).
 Alternatively, we often find it more convenient to calculate log probability
 
\begin_inset Formula $\ln p(v)$
\end_inset

.
 To do this, we would use equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LogBernoulli"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:LogChain"

\end_inset

) instead.
\end_layout

\begin_layout Standard
Gradients of the loss function with respect to each of the parameters are
 used in the back propagation algorithm.
 We choose the negative log probability 
\begin_inset Formula $-\ln p(v)$
\end_inset

 as the loss function.
 We can first break the calculation up using the chain rule with 
\begin_inset Formula 
\begin{eqnarray*}
-\frac{\partial\ln p(v)}{\partial x} & = & -\sum_{i=1}^{n}\frac{\partial}{\partial x}\Big(v_{i}\ln p_{i}(x)+(1-v_{i})\ln(1-p_{i}(x))\Big)\\
 & = & \sum_{i=1}^{n}\frac{p_{i}-v_{i}}{p_{i}(1-p_{i})}\frac{\partial p_{i}(x)}{\partial x},
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $x$
\end_inset

 is a variable associated with the top layer of the network and 
\begin_inset Formula $\delta_{ij}$
\end_inset

 is the dirac-
\begin_inset Formula $\delta$
\end_inset

 function 
\begin_inset Formula 
\[
\delta_{ij}=\begin{cases}
1 & \text{ if \ensuremath{i=j}, and}\\
0 & \text{otherwise.}
\end{cases}
\]

\end_inset

We note that to back propagate derivites to subsequent layers we will have
 to expand the 
\begin_inset Formula $\partial p/\partial x$
\end_inset

 term further.
\end_layout

\begin_layout Standard
We require gradients for each of the parameters in the network and denote
 these by 
\begin_inset Formula $\Delta b^{(i)}$
\end_inset

, 
\begin_inset Formula $\Delta V_{j}^{(i)}$
\end_inset

, 
\begin_inset Formula $\Delta c_{i}$
\end_inset

 and 
\begin_inset Formula $\Delta W_{ij}$
\end_inset

.
 As an intermediate step, it is also useful to compute gradients associated
 with the hidden unit 
\begin_inset Formula $\Delta h_{j}^{(i)}$
\end_inset

.
 Finally we also specify that the activation function 
\begin_inset Formula $\sigma$
\end_inset

 for this model is the sigmoid function, for which we can use the identity
 
\begin_inset Formula $d\sigma(y)/dx=\sigma(1-\sigma)dy/dx$
\end_inset

.
 
\end_layout

\begin_layout Standard
We begin the calculation with 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta b^{(i)} & = & -\frac{\partial\ln p(v)}{\partial b^{(i)}}\\
 & = & \sum_{j=1}^{n}\frac{p_{j}-v_{j}}{p_{j}(1-p_{j})}\frac{\partial p_{j}}{\partial b^{(i)}}\\
 & = & \sum_{j=1}^{n}(p_{j}-v_{j})\delta_{ij}\\
 & = & p_{i}-v_{i},
\end{eqnarray*}

\end_inset

and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta V_{j}^{(i)} & = & -\frac{\partial\ln p(v)}{\partial V_{j}^{(i)}}\\
 & = & \sum_{k=1}^{n}\frac{p_{k}-v_{k}}{p_{k}(1-p_{k})}\frac{\partial p_{k}}{\partial V_{j}^{(i)}}\\
 & = & \sum_{k=1}^{n}\sum_{l=1}^{m}(p_{k}-v_{k})h_{l}^{(k)}\delta_{ik}\delta_{jl}\\
 & = & (p_{i}-v_{i})h_{j}^{(i)}=\Delta b^{(i)}h_{j}^{(i)}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The calculation of 
\begin_inset Formula $\Delta h_{j}^{(i)}$
\end_inset

 is similar to that for 
\begin_inset Formula $\Delta V_{j}^{(i)}$
\end_inset

 with
\begin_inset Formula 
\[
\Delta h_{j}^{(i)}=-\frac{\partial\ln p(v)}{\partial h_{j}^{(i)}}=\Delta b^{(i)}V_{j}^{(i)}.
\]

\end_inset


\end_layout

\begin_layout Standard
We can now calculate gradients associated with the hidden layer
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta c_{i} & = & -\frac{\partial\ln p(v)}{\partial c_{i}}\\
 & = & -\sum_{j=1}^{n}\sum_{k=1}^{m}\frac{\partial\ln p(v)}{\partial h_{k}^{(j)}}\frac{\partial h_{k}^{(j)}}{\partial c_{i}}\\
 & = & \sum_{j=1}^{n}\sum_{k=1}^{m}\Delta h_{k}^{(j)}\frac{\partial h_{k}^{(j)}}{\partial c_{i}}\\
 & = & \sum_{j=1}^{n}\sum_{k=1}^{m}\Delta h_{k}^{(j)}h_{k}^{(j)}(1-h_{k}^{(j)})\delta_{ik}\\
 & = & \sum_{j=1}^{n}\Delta h_{i}^{(j)}h_{i}^{(j)}(1-h_{i}^{(j)}),
\end{eqnarray*}

\end_inset

and 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\Delta W_{ij} & = & -\frac{\partial\ln p(v)}{\partial W_{ij}}\\
 & = & \sum_{k=1}^{n}\sum_{l=1}^{m}\Delta h_{l}^{(k)}\frac{\partial h_{l}^{(k)}}{\partial W_{ij}}\\
 & = & \sum_{k=1}^{n}\sum_{l=1}^{m}\Delta h_{l}^{(k)}h_{l}^{(k)}(1-h_{l}^{(k)})\frac{\partial}{\partial W_{ij}}\sum_{q=1}^{k-1}W_{lq}v_{q}\\
 & = & \sum_{k=1}^{n}\sum_{l=1}^{m}\sum_{q=1}^{k-1}\Delta h_{l}^{(k)}h_{l}^{(k)}(1-h_{l}^{(k)})v_{q}\delta_{il}\delta_{jq}\\
 & = & \sum_{k=1}^{n}\sum_{q=1}^{k-1}\Delta h_{i}^{(k)}(1-h_{i}^{(k)})h_{i}^{(k)}v_{q}\delta_{jq}\\
 & = & \sum_{q=1}^{n}\sum_{k=q+1}^{n}\Delta h_{i}^{(k)}(1-h_{i}^{(k)})h_{i}^{(k)}v_{q}\delta_{jq}\\
 & = & \sum_{k=j+1}^{n}\Delta h_{i}^{(k)}(1-h_{i}^{(k)})h_{i}^{(k)}v_{j},
\end{eqnarray*}

\end_inset

where we have used the trick
\begin_inset Formula 
\[
\sum_{i=1}^{n}\sum_{j=1}^{i-1}a_{i}b_{j}=\sum_{j=1}^{n}\sum_{i=j+1}^{n}a_{i}b_{j}
\]

\end_inset

to exchange summations.
 As seen in these equations, to calculate gradients we must first do a forward
 pass with 
\begin_inset Formula $v_{i}$
\end_inset

 to compute values for both conditional probabilities 
\begin_inset Formula $p_{i}$
\end_inset

 as well as values for the the hidden layer 
\begin_inset Formula $h_{j}^{(i)}$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection*
Errors and improvements to the Larochelle and Murray paper: The Neural Autoregre
ssive Distribution Estimator
\end_layout

\begin_layout Standard
In the Algorithm 1, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $p(v)\leftarrow0$
\end_inset

 should read 
\begin_inset Formula $p(v)\leftarrow1$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p(v)\leftarrow p(v)\big(p(v_{i}=1|\mathbf{v}_{<i})^{v_{i}}+(1-p(v_{i}=1|\mathbf{v}_{<i}))^{1-v_{i}}\big)$
\end_inset

 should read 
\begin_inset Formula $p(v)\leftarrow p(v)\big(p(v_{i}=1|\mathbf{v}_{<i})^{v_{i}}(1-p(v_{i}=1|\mathbf{v}_{<i}))^{1-v_{i}}\big)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Calculating 
\begin_inset Formula $\delta\mathbf{c}$
\end_inset

 is redundant.
 Just calculate 
\begin_inset Formula $\delta\mathbf{a}$
\end_inset

 within the loop as specified and after that set 
\begin_inset Formula $\delta\mathbf{c}\leftarrow\delta\mathbf{a}$
\end_inset

.
\end_layout

\begin_layout Itemize
The term 
\begin_inset Formula $(\delta\mathbf{h}_{i})\mathbf{h}_{i}(1-\mathbf{h}_{i})$
\end_inset

 refers to a element by element product given the index 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Section
kNade
\end_layout

\begin_layout Standard
The NADE model is a fully autoregressive model.
 We now consider the case where we impose a k-Markov condition in place
 of full autoregressivity.
 That is, we will assume that 
\begin_inset Formula $p(v_{i}\vert v_{<i})\approx p(v_{i}|v_{i-1}v_{i-2}\cdots v_{i-k})$
\end_inset

.
 The joint probability distribution is now given by 
\begin_inset Formula 
\begin{equation}
-\ln p(v)=-\sum_{i=1}^{n}\ln p(v_{i}\vert v_{i-1}v_{i-2}\cdots v_{i-k}),\label{eq:LogkMarkovChain}
\end{equation}

\end_inset

where we ignore the variable 
\begin_inset Formula $v_{i}$
\end_inset

 whenever 
\begin_inset Formula $i\le0$
\end_inset

.
 The construction of the kNADE model proceeds similarly and the only difference
 is in the lowest layer, linking the visible units to the first layer of
 hidden units.
 The gradient calculation for the bias 
\begin_inset Formula $\Delta c_{i}$
\end_inset

 is identical, but the calculation for the weight now reads 
\begin_inset Formula 
\[
\Delta W_{ij}=\sum_{q=j+1}^{j+1+k}\Delta h_{i}^{(q)}(1-h_{i}^{(q)})h_{i}^{(q)}v_{j}.
\]

\end_inset


\end_layout

\begin_layout Subsection*
Acknowledgements
\end_layout

\begin_layout Standard
Thanks to Sébastien Racaniere for his helpful comments.
\end_layout

\end_body
\end_document
